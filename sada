Alright â€” now weâ€™re cooking ğŸ³
You want the same **LectaVera backend plan** but updated to match your new infra:

> ğŸ’¸ **100% Free Architecture**
> API â†’ Render
> DB â†’ Render Postgres
> Redis â†’ Upstash
> Vector â†’ Pinecone
> Files â†’ Supabase / Cloudinary

And youâ€™re **not using Celery**, so Redis is for:

* Caching RAG results
* Chat session memory
* WebSocket pub/sub
* Rate limiting
* Processing status tracking
* Temporary tokens / OTP / reset links

Letâ€™s update your architecture cleanly ğŸ‘‡

---

# ğŸ§  LectaVera Backend â€” Updated Architecture (With Redis)

## Updated Tech Stack

### Backend Core

* **FastAPI**
* **PostgreSQL (Render)** â†’ persistent data
* **Upstash Redis** â†’ cache + realtime + temp state
* **Supabase Storage** â†’ PDFs
* **Pinecone** â†’ embeddings
* **LangGraph** â†’ CRAG workflow
* **OpenAI / Groq** â†’ LLM
* **Tavily** â†’ web search

---

# ğŸ†• Redis Responsibilities (New)

We re-introduce Redis but only for **ephemeral / fast data**:

| Feature                    | Stored In |
| -------------------------- | --------- |
| JWT blacklist              | Redis     |
| Refresh tokens             | Postgres  |
| Chat live memory           | Redis     |
| Chat history archive       | Postgres  |
| Document processing status | Redis     |
| RAG cache                  | Redis     |
| WebSocket pub/sub          | Redis     |
| Rate limiting              | Redis     |

Think:

> Postgres = permanent brain
> Redis = short-term memory âš¡

---

# ğŸ“¦ Updated Requirements.txt

Add Redis client:

```txt
redis==5.0.1
hiredis==2.3.2  # Faster parsing (optional)
```

(Everything else same as your current file.)

---

# ğŸ” Environment Variables (Updated)

```env
# Redis (Upstash)
REDIS_URL=redis://default:password@host:port
REDIS_CACHE_TTL=3600
REDIS_CHAT_TTL=86400
REDIS_RATE_LIMIT=100
```

Upstash gives this URL directly.

---

# ğŸ§© New Project Structure Changes

Add Redis utilities:

```
app/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ redis.py          # Redis connection
â”œâ”€â”€ services/
â”‚   â””â”€â”€ cache_service.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ rate_limiter.py
```

---

# 1ï¸âƒ£ Redis Connection

**app/core/redis.py**

```python
import redis.asyncio as redis
from app.core.config import settings

redis_client = redis.from_url(
    settings.REDIS_URL,
    encoding="utf-8",
    decode_responses=True
)
```

Async client works best on Render.

---

# 2ï¸âƒ£ RAG Response Caching

Why?

LLM + Pinecone = expensive + slow.

Cache repeated queries.

**app/services/cache_service.py**

```python
import hashlib
import json
from app.core.redis import redis_client

CACHE_TTL = 3600  # 1 hour


def _hash_query(query: str, user_id: str) -> str:
    raw = f"{user_id}:{query}"
    return hashlib.md5(raw.encode()).hexdigest()


async def get_cached_rag(query: str, user_id: str):
    key = f"rag:{_hash_query(query, user_id)}"
    data = await redis_client.get(key)
    return json.loads(data) if data else None


async def set_cached_rag(query: str, user_id: str, response: dict):
    key = f"rag:{_hash_query(query, user_id)}"
    await redis_client.setex(
        key,
        CACHE_TTL,
        json.dumps(response)
    )
```

---

## Update RAG Service

```python
from app.services.cache_service import (
    get_cached_rag,
    set_cached_rag
)

async def process_query(...):

    cached = await get_cached_rag(question, user_id)
    if cached:
        return cached

    result = self.graph.invoke(state)

    await set_cached_rag(question, user_id, result)

    return result
```

âš¡ Saves tokens + Pinecone calls.

---

# 3ï¸âƒ£ Document Processing Status (Realtime)

Instead of polling DB constantly:

Store status in Redis.

### Update background processor

```python
from app.core.redis import redis_client

status_key = f"doc_status:{doc_id}"

await redis_client.set(status_key, "processing")

# After completion
await redis_client.set(status_key, "ready")
```

### API endpoint

```python
@router.get("/{document_id}/status")
async def get_status(document_id: str):

    status = await redis_client.get(
        f"doc_status:{document_id}"
    )

    return {
        "status": status or "processing"
    }
```

Faster than DB polling.

---

# 4ï¸âƒ£ Chat Live Memory (WebSocket)

Store last N messages in Redis.

```python
key = f"chat_live:{session_id}"

await redis_client.rpush(key, json.dumps(message))
await redis_client.expire(key, 86400)
```

When session ends â†’ move to Postgres archive.

---

# 5ï¸âƒ£ Rate Limiting

**app/utils/rate_limiter.py**

```python
from fastapi import HTTPException
from app.core.redis import redis_client

LIMIT = 100
WINDOW = 60  # seconds


async def rate_limit(user_id: str):

    key = f"rate:{user_id}"

    count = await redis_client.incr(key)

    if count == 1:
        await redis_client.expire(key, WINDOW)

    if count > LIMIT:
        raise HTTPException(
            429,
            "Rate limit exceeded"
        )
```

Use in dependencies.

---

# 6ï¸âƒ£ JWT Blacklist (Logout)

```python
async def blacklist_token(jti: str):
    await redis_client.setex(
        f"jwt_blacklist:{jti}",
        3600,
        "true"
    )
```

Check on every request.

---

# ğŸ§± Updated Deployment Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Frontend    â”‚
                â”‚  (Vercel)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ FastAPI API  â”‚
               â”‚   Render     â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼                â–¼                â–¼
Postgres         Upstash Redis     Pinecone
(Render)          (Cache/RT)        (Vectors)

                      â”‚
                      â–¼
               Supabase Storage
                    (PDFs)
```

---

# ğŸ†• Updated Infra Summary

| Component | Service         | Purpose          |
| --------- | --------------- | ---------------- |
| API       | Render          | Backend          |
| DB        | Render Postgres | Persistent data  |
| Redis     | Upstash         | Cache + realtime |
| Vector DB | Pinecone        | Embeddings       |
| Files     | Supabase        | PDFs             |
| LLM       | OpenAI/Groq     | Generation       |

Cost â†’ **$0 (free tiers)** ğŸ’¸

---

# ğŸ§­ What Changed From Your Old Plan

### Added back

* Redis caching
* Redis rate limiting
* Redis pub/sub
* Redis doc status tracking

### Still removed

* Celery
* Workers
* Message brokers

So infra stays simple.

---

# ğŸš€ When to scale later

Upgrade path:

| Need            | Add           |
| --------------- | ------------- |
| Heavy jobs      | Celery        |
| Long PDFs       | Workers       |
| Streaming scale | Redis cluster |
| Analytics       | ClickHouse    |

